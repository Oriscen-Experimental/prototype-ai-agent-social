# Backend environment variables (prototype)

# CORS
ALLOWED_ORIGINS=*

# If you later enable real LLM calls, align with ai-service:
# DEFAULT_AI_MODEL=grok-4-1-fast-non-reasoning
# XAI_API_KEY=...
# XAI_BASE_URL=https://api.x.ai/v1
# OPENAI_API_KEY=...
# OPENAI_BASE_URL=https://api.openai.com/v1

# Routing behavior
# If true, the router/search will call a real LLM when API keys exist.
# Otherwise everything is deterministic mock generation.
ENABLE_REAL_LLM=false

# Log level (debug/info/warn/error)
LOG_LEVEL=info

# In-memory session TTL (prototype). Note: Render may restart/scale instances, so sessions aren't durable.
SESSION_TTL_SECONDS=21600
